---
title: "Asking Questions (Learn-style, Experimental)"
id: "module-01L"
domainSlug: "asking-questions-learn"
domainEnum: ASKING_QUESTIONS
difficulty: "Beginner"
estimatedTime: "60 min"
blueprintWeight: 0.22
description: "Rapid microlearning path for mapping intent to Tanium questions, validating sensor choice, and operationalizing saved questions."
objectives:
  - "Connect stakeholder intent to Tanium question structure within minutes"
  - "Select and validate sensors for inventory, hygiene, and security scenarios"
  - "Apply safe targeting patterns and communicate assumptions"
  - "Summarize, share, and automate question outcomes"
tags: ["microlearning", "experiment", "learn-style", "asking-questions"]
status: "draft"
version: 1
lastUpdated: "2025-10-05"
---


const unit01QuickCheck = [
  {
    id: "module-01L-unit-01-q1",
    moduleId: "module-01L",
    sectionId: "unit-01L-01",
    concept: "Intent Mapping",
    question: "You receive the request 'Find laptops that haven't reported VPN activity this week.' What should you clarify before drafting the Tanium question?",
    type: "multiple-choice",
    options: [
      "The exact VPN sensor output format and acceptable inactive window",
      "Whether the VPN solution is from Microsoft or Cisco",
      "If the requester prefers CSV or JSON export",
      "Which computer group owns laptop patching"
    ],
    correctAnswer: "The exact VPN sensor output format and acceptable inactive window",
    explanation:
      "Intent-first querying starts by validating outcomes: confirm which sensor exposes VPN activity and the timeframe that defines 'inactive' before constructing the prompt.",
    difficulty: "medium",
    tags: ["intent", "question-framing"]
  },
  {
    id: "module-01L-unit-01-q2",
    moduleId: "module-01L",
    sectionId: "unit-01L-01",
    concept: "Stakeholder Alignment",
    question: "Why capture a follow-up variation when drafting an initial question prompt?",
    type: "multiple-choice",
    options: [
      "To have a backup ready if the original query times out",
      "To clarify the operational intent and accelerate refinement after first results",
      "To reduce licensing cost by batching questions",
      "To satisfy Tanium's requirement for dual prompts"
    ],
    correctAnswer: "To clarify the operational intent and accelerate refinement after first results",
    explanation:
      "A follow-up variation documents how you might iterate based on results, keeping the conversation tied to the business outcome instead of improvising later.",
    difficulty: "easy",
    tags: ["intent", "iteration"]
  }
];

const unit02QuickCheck = [
  {
    id: "module-01L-unit-02-q1",
    moduleId: "module-01L",
    sectionId: "unit-01L-02",
    concept: "Sensor Selection",
    question: "Which sensor pair best validates Windows update hygiene for executive laptops?",
    type: "multiple-choice",
    options: [
      "Installed Applications & IP Address",
      "Patch Status & Last Logged In User",
      "Windows Update Status & Pending Actions",
      "Running Services & CPU Utilization"
    ],
    correctAnswer: "Windows Update Status & Pending Actions",
    explanation:
      "The duo surfaces compliance posture and outstanding remediation steps, aligning with stakeholder expectations for update hygiene.",
    difficulty: "medium",
    tags: ["sensors", "hygiene"]
  },
  {
    id: "module-01L-unit-02-q2",
    moduleId: "module-01L",
    sectionId: "unit-01L-02",
    concept: "Result Fidelity",
    question: "Before scheduling a fleet-wide question, what sensor detail prevents misinterpretation of results?",
    type: "multiple-choice",
    options: [
      "Default chart color",
      "Data type and sample output",
      "Sensor owner",
      "Sensor creation date"
    ],
    correctAnswer: "Data type and sample output",
    explanation:
      "Previewing sensor output confirms whether values arrive as strings, counts, or booleans and avoids downstream parsing issues.",
    difficulty: "easy",
    tags: ["sensor-preview"]
  }
];

const unit03QuickCheck = [
  {
    id: "module-01L-unit-03-q1",
    moduleId: "module-01L",
    sectionId: "unit-01L-03",
    concept: "Targeting",
    question: "A question needs to hit only finance endpoints after hours. Which targeting pattern keeps results actionable without risking production systems?",
    type: "multiple-choice",
    options: [
      "Run from all machines then filter in Excel",
      "Use the Finance dynamic group and schedule during the maintenance window",
      "Clone the question and assign to every user",
      "Convert the question to a package"
    ],
    correctAnswer: "Use the Finance dynamic group and schedule during the maintenance window",
    explanation:
      "Dynamic groups plus change-window alignment reduce noise and match the stakeholder's blast radius requirements.",
    difficulty: "medium",
    tags: ["targeting", "change-window"]
  },
  {
    id: "module-01L-unit-03-q2",
    moduleId: "module-01L",
    sectionId: "unit-01L-03",
    concept: "Assumptions",
    question: "Why document targeting assumptions alongside the question?",
    type: "multiple-choice",
    options: [
      "It enables Tanium to auto-generate dynamic groups",
      "It supports auditability and helps teammates understand the data slice",
      "It shortens question execution time",
      "It unlocks visualizations in Trends"
    ],
    correctAnswer: "It supports auditability and helps teammates understand the data slice",
    explanation:
      "Capturing assumptions keeps future reviewers aligned on the population being queried and simplifies downstream troubleshooting.",
    difficulty: "easy",
    tags: ["documentation"]
  }
];

const unit04QuickCheck = [
  {
    id: "module-01L-unit-04-q1",
    moduleId: "module-01L",
    sectionId: "unit-01L-04",
    concept: "Result Story",
    question: "What is the first step when transforming raw question output into an exec-ready update?",
    type: "multiple-choice",
    options: [
      "Export to PDF",
      "Highlight the headline trend or anomaly",
      "Archive the question",
      "Share with all RBAC roles"
    ],
    correctAnswer: "Highlight the headline trend or anomaly",
    explanation:
      "Stakeholders need the storyline-surface the key change before diving into raw tables.",
    difficulty: "easy",
    tags: ["communication"]
  },
  {
    id: "module-01L-unit-04-q2",
    moduleId: "module-01L",
    sectionId: "unit-01L-04",
    concept: "Saved Questions",
    question: "Which saved-question signal warns you to refresh or retire it before sharing?",
    type: "multiple-choice",
    options: [
      "The question name is longer than 30 characters",
      "Execution status shows stale data or repeated failures",
      "It uses more than two sensors",
      "It has more than five subscribers"
    ],
    correctAnswer: "Execution status shows stale data or repeated failures",
    explanation:
      "Health indicators highlight when results are no longer trustworthy-address them before presenting insights.",
    difficulty: "medium",
    tags: ["saved-questions", "health"]
  }
];

const unit05QuickCheck = [
  {
    id: "module-01L-unit-05-q1",
    moduleId: "module-01L",
    sectionId: "unit-01L-05",
    concept: "Automation",
    question: "Before scheduling a recurring question, what validation keeps automation safe?",
    type: "multiple-choice",
    options: [
      "Ensuring the question uses fewer than three filters",
      "Running the question in a canary group and reviewing impact",
      "Assigning ownership to every analyst",
      "Exporting results to CSV"
    ],
    correctAnswer: "Running the question in a canary group and reviewing impact",
    explanation:
      "A canary or pilot run validates performance and data quality before automating at scale.",
    difficulty: "medium",
    tags: ["automation", "canary"]
  },
  {
    id: "module-01L-unit-05-q2",
    moduleId: "module-01L",
    sectionId: "unit-01L-05",
    concept: "Playbook",
    question: "What should you capture in the question improvement log?",
    type: "multiple-choice",
    options: [
      "Only the final query string",
      "Inputs, outputs, and the next action or owner",
      "Sensor creation dates",
      "Names of everyone who viewed the dashboard"
    ],
    correctAnswer: "Inputs, outputs, and the next action or owner",
    explanation:
      "Recording the full loop helps future reviewers understand what changed and who drives the follow-up.",
    difficulty: "easy",
    tags: ["playbook", "documentation"]
  }
];

const summaryQuickCheck = [
  {
    id: "module-01L-summary-q1",
    moduleId: "module-01L",
    sectionId: "summary",
    concept: "Question Pattern",
    question: "Which natural-language structure keeps Tanium questions readable and executable?",
    type: "multiple-choice",
    options: [
      "Find [sensor] in [targets] when [filters]",
      "Get [sensor] from [targets] where [filters]",
      "Run [sensor] for [filters] on [targets]",
      "Select [sensor] across [targets] during [time]"
    ],
    correctAnswer: "Get [sensor] from [targets] where [filters]",
    explanation:
      "Tanium compiles human-friendly prompts that follow the Get/From/Where pattern, making them easy to test and share.",
    difficulty: "easy",
    tags: ["syntax"]
  },
  {
    id: "module-01L-summary-q2",
    moduleId: "module-01L",
    sectionId: "summary",
    concept: "Sensor Preview",
    question: "When evaluating a new sensor, which action confirms it returns the values you expect?",
    type: "multiple-choice",
    options: [
      "Reviewing the sensor description only",
      "Running a preview on a known-good endpoint",
      "Checking the default dashboard",
      "Asking another admin for validation"
    ],
    correctAnswer: "Running a preview on a known-good endpoint",
    explanation:
      "Previewing against a trusted endpoint shows how the values render and whether formatting matches stakeholder needs.",
    difficulty: "medium",
    tags: ["sensors"]
  },
  {
    id: "module-01L-summary-q3",
    moduleId: "module-01L",
    sectionId: "summary",
    concept: "Targeting",
    question: "Which statement best describes safe targeting?",
    type: "multiple-choice",
    options: [
      "Run every question against All Machines to ensure completeness",
      "Align dynamic groups and scheduling with change-management windows",
      "Copy saved questions between users without review",
      "Avoid documenting targeting logic to keep questions lightweight"
    ],
    correctAnswer: "Align dynamic groups and scheduling with change-management windows",
    explanation:
      "Targeting is safest when filtered to the right population at the right time, then documented for future audits.",
    difficulty: "medium",
    tags: ["targeting"]
  },
  {
    id: "module-01L-summary-q4",
    moduleId: "module-01L",
    sectionId: "summary",
    concept: "Result Sharing",
    question: "How should you present a real-time question outcome to an executive sponsor?",
    type: "multiple-choice",
    options: [
      "Send the raw table immediately",
      "Lead with the insight, then provide the saved question link",
      "Wait for the next monthly review",
      "Export to Excel and remove filters"
    ],
    correctAnswer: "Lead with the insight, then provide the saved question link",
    explanation:
      "Share the headline first and offer the supporting data source for transparency.",
    difficulty: "easy",
    tags: ["communication"]
  },
  {
    id: "module-01L-summary-q5",
    moduleId: "module-01L",
    sectionId: "summary",
    concept: "Automation",
    question: "What should happen before a question graduates into automation?",
    type: "multiple-choice",
    options: [
      "It is copied into every dashboard",
      "It is validated in a pilot group with success criteria documented",
      "It runs successfully once on All Machines",
      "It receives at least three hint requests"
    ],
    correctAnswer: "It is validated in a pilot group with success criteria documented",
    explanation:
      "Automation requires a repeatable, low-risk question. Validate impact and capture expectations first.",
    difficulty: "medium",
    tags: ["automation"]
  }
];

# Asking Questions (Learn Experimental Track)

Kick-start the experimental Learn-style path by grounding every question in a clear objective. Move from intent to execution in repeatable five-to-ten minute units that build mastery quickly.

<MicroSection
  id="unit-01L-01"
  moduleId="module-01L"
  title="Intent-Driven Querying"
  estimatedMinutes={10}
  sectionNumber={1}
  totalSections={5}
  keyTakeaways={[
    "Start with the outcome before drafting any Tanium question.",
    "Use natural-language framing to move from idea to execution without friction.",
    "Validate the question prompt against the data stakeholders expect."
  ]}
  quickCheck={<MicroQuizMDX moduleId="module-01L" unitId="unit-01L-01" quizIdPrefix="module-01L-unit-01" questions={unit01QuickCheck} />}
>

### Learning outcomes

- Connect business intent to Tanium question structure in under two minutes.
- Identify when to refine the prompt versus adjust targeting.

### Guided walkthrough

1. Capture the stakeholder intent in one sentence.
2. Identify the signal you need and confirm the sensor that exposes it.
3. Reframe the sentence into Tanium's **Get / From / Where** pattern.

<QueryPlayground
  title="Intent → Question"
  instruction="Identify laptops missing the CrowdStrike Falcon sensor in the last 24 hours."
  expectedQuery="Get Computer Name from all machines where CrowdStrike Falcon Sensor contains \"Not Installed\""
  expectedResult="LAPTOP-FIN-102
LAPTOP-FIN-221
LAPTOP-SALES-014
... (showing 38 of 2,143 results)"
  hint="Translate the intent into 'Get [signal] from [target] where [condition]' and verify the sensor name matches documentation."
  difficulty="beginner"
/>

<PracticeButton
  href="/study/labs/01-l/intent-to-question"
  aria-label="Open the intent-to-question practice lab"
>
  Practice: Draft Intent-First Questions
</PracticeButton>

</MicroSection>

<MicroSection
  id="unit-01L-02"
  moduleId="module-01L"
  title="Selecting Sensors with Confidence"
  estimatedMinutes={12}
  sectionNumber={2}
  totalSections={5}
  keyTakeaways={[
    "Align sensors with the data story: inventory, hygiene, or threat visibility.",
    "Use sensor previews and documentation to confirm output format.",
    "Bookmark go-to sensors for repeated operational quests."
  ]}
  quickCheck={<MicroQuizMDX moduleId="module-01L" unitId="unit-01L-02" quizIdPrefix="module-01L-unit-02" questions={unit02QuickCheck} />}
>

### Learning outcomes

- Choose the best-fit sensor for asset, configuration, or security investigations.
- Confirm result fidelity and payload details before running fleet scale questions.

### Guided activity

- Compare two similar sensors (for example, **Installed Applications** vs **Applications - Add/Remove Programs**) and note which produces actionable output faster.
- Preview sensor output on a controlled endpoint, recording the data type, sample values, and any normalization needed.

<QueryPlayground
  title="Sensor Dry Run"
  instruction="Preview disk utilization across production servers before sharing results."
  expectedQuery="Get Disk Space from Computer Group[Production Servers]"
  expectedResult="SERVER-DB-01 - C:\\ 12% Free
SERVER-API-03 - C:\\ 28% Free
SERVER-WEB-07 - C:\\ 8% Free
..."
  hint="Pair sensor previews with targeted groups to validate column structure and threshold triggers."
  difficulty="intermediate"
/>

<PracticeButton
  href="/study/labs/01-l/sensor-shortlist"
  aria-label="Open the sensor shortlist practice lab"
>
  Practice: Build a Sensor Shortlist
</PracticeButton>

</MicroSection>

<MicroSection
  id="unit-01L-03"
  moduleId="module-01L"
  title="Targeting that Reduces Noise"
  estimatedMinutes={12}
  sectionNumber={3}
  totalSections={5}
  keyTakeaways={[
    "Leverage dynamic groups to isolate actionable endpoint sets.",
    "Limit the question scope before scheduling recurring queries.",
    "Document targeting assumptions for handoff and auditability."
  ]}
  quickCheck={<MicroQuizMDX moduleId="module-01L" unitId="unit-01L-03" quizIdPrefix="module-01L-unit-03" questions={unit03QuickCheck} />}
>

### Learning outcomes

- Apply filters and computer groups to keep results crisp.
- Align targeting choices with change windows and risk appetite.

### Decision checklist

- Can the sensor run against the audience without performance impact?
- Does a dynamic or role-based group exist to narrow scope?
- What change window or maintenance period keeps operations safe?
- Which assumptions (OS, privileges, business unit) should be documented alongside the saved question?

<PracticeButton
  href="/study/labs/01-l/targeting-checklist"
  aria-label="Open the targeting checklist practice lab"
>
  Practice: Build a Targeting Checklist
</PracticeButton>

</MicroSection>

<MicroSection
  id="unit-01L-04"
  moduleId="module-01L"
  title="Reading and Sharing Live Results"
  estimatedMinutes={10}
  sectionNumber={4}
  totalSections={5}
  keyTakeaways={[
    "Sort and pivot real-time data to elevate the headline insight.",
    "Use saved question visual cues to show question health.",
    "Capture anomalies immediately for downstream action teams."
  ]}
  quickCheck={<MicroQuizMDX moduleId="module-01L" unitId="unit-01L-04" quizIdPrefix="module-01L-unit-04" questions={unit04QuickCheck} />}
>

### Learning outcomes

- Summarize response data in language stakeholders understand.
- Spot stale or unhealthy saved questions before presenting results.

### Collaboration steps

1. Capture the raw response table and annotate the top insight (e.g., "12% of finance laptops are offline").
2. Use column filters to isolate outliers or trend lines.
3. Validate saved-question health (last run timestamp, error indicators) before sharing the link.
4. Draft a two-sentence stakeholder update that states the impact and recommended next step.

<QueryPlayground
  title="Tell the Story"
  instruction="Identify machines that have not checked into Tanium in 7 days and craft an executive summary."
  expectedQuery="Get Computer Name from all machines where Last Check-in time <= now-7d"
  expectedResult="SERVER-API-03 - 8 days
LAPTOP-FIN-221 - 9 days
LAPTOP-SALES-014 - 15 days
... (showing 112 of 5,432 results)"
  hint="Pair the query with a short narrative: what changed, why it matters, and who should act."
  difficulty="intermediate"
/>

<PracticeButton
  href="/study/labs/01-l/share-results"
  aria-label="Open the share results practice lab"
>
  Practice: Snapshot & Share
</PracticeButton>

</MicroSection>

<MicroSection
  id="unit-01L-05"
  moduleId="module-01L"
  title="Iterate and Automate"
  estimatedMinutes={11}
  sectionNumber={5}
  totalSections={5}
  keyTakeaways={[
    "Schedule recurring questions only after validating impact.",
    "Translate learning into tags, dashboards, or follow-up actions.",
    "Log question improvements for next run’s playbook."
  ]}
  quickCheck={<MicroQuizMDX moduleId="module-01L" unitId="unit-01L-05" quizIdPrefix="module-01L-unit-05" questions={unit05QuickCheck} />}
>

### Learning outcomes

- Turn repeatable question patterns into reusable automation assets.
- Record improvement insights to brief operations teams quickly.

### Build the habit

- Run a canary execution and document run time, results, and impact.
- Record which tags, action groups, or integrations should receive the results automatically.
- Capture next actions, owners, and timelines in a living playbook.

<PracticeButton
  href="/study/labs/01-l/iterate-fast"
  aria-label="Open the iterate fast practice lab"
>
  Practice: Automate Next Steps
</PracticeButton>

</MicroSection>

## End-module knowledge check

Reinforce the story by answering a wider mix of core questions.

<MicroQuizMDX
  moduleId="module-01L"
  unitId="unit-01L-summary"
  quizIdPrefix="module-01L-summary"
  questions={summaryQuickCheck}
/>

> ✅ Ready to experiment with the rest of the Learn-style path? Continue with Refining Questions & Targeting (02-L).
